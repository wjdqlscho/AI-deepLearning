{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Device 설정\n",
        "- 일반적으로 인공신경망의 학습은 (가능하다면) GPU를 사용하는 것이 바람직함\n",
        "    - Colab Runtime 설정 변경\n",
        "- GPU를 사용하여 학습을 진행하도록 명시적으로 작성 필요\n",
        "- 연산 유형에 따라 GPU에서 수행이 불가능한 경우도 존재하는데, 그럴 경우도 마찬가지로 명시적으로 어떤 프로세서에서 연산을 수행해야하는지 코드로 작성해야함\n",
        "\n",
        "```\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = NeuralNetwork().to(device)\n",
        "```\n"
      ],
      "metadata": {
        "id": "e0H3duTdrqi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. 신경망 생성\n",
        "\n",
        "- **torch.nn 패키지**는 신경망 생성 및 학습 시 설정해야하는 다양한 기능을 제공\n",
        "\n",
        "```\n",
        "import torch.nn as nn\n",
        "```\n",
        "- 신경망을 **nn.Module**을 상속받아 정의\n",
        "    - __ __init__ __(): 신경망에서 사용할 layer를 초기화하는 부분\n",
        "    - __forward()__: feed foward 연산 수행 시, 각 layer의 입출력이 어떻게 연결되는지를 지정\n",
        "\n",
        "```\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.input_layer    = nn.Linear(4, 16)\n",
        "        self.hidden_layer1  = nn.Linear(16, 32)\n",
        "        self.output_layer   = nn.Linear(32, 3)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out =  self.relu(self.input_layer(x))\n",
        "        out =  self.relu(self.hidden_layer1(out))\n",
        "        out =  self.output_layer(out)\n",
        "        return out\n",
        "\n",
        "```\n"
      ],
      "metadata": {
        "id": "IvGB8XKLrndB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model compile\n",
        "\n",
        "- 학습 시 필요한 정보들(loss function, optimizer)을 선언\n",
        "- 일반적으로 loss와 optimizer는 아래와 같이 변수로 선언하고, 변수를 train/test 시 참고할 수 있도록 매개변수로 지정해줌\n",
        "\n",
        "```\n",
        "learning_rate = 0.01\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "```\n"
      ],
      "metadata": {
        "id": "kdaqKz_DrveR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Train\n",
        "- **신경망의 학습과정**을 별도의 함수로 구성하는 것이 일반적\n",
        "    - feed forward -> loss -> error back propagation -> (log) -> (반복)\n",
        "\n",
        "```\n",
        "def train_loop(train_loader, model, loss_fn, optimizer):\n",
        "    for batch, (X, y) in enumerate(train_loader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "        optimizer.zero_grad() 옵티마이저 초기화 하는 거임\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "```"
      ],
      "metadata": {
        "id": "6QL46lI-ryBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Test\n",
        "\n",
        "- 학습과정과 비슷하나 error back propagate하는 부분이 없음\n",
        "    - feed forward -> loss ->  (log) -> (반복)\n",
        "\n",
        "```\n",
        "def test_loop(test_loader, model, loss_fn):\n",
        "    size = len(test_loader.dataset)\n",
        "    num_batches = len(test_loader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:8f}\\n\")\n"
      ],
      "metadata": {
        "id": "366tXVkAr93J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Iteration\n",
        "- 신경망 학습은 여러 epochs을 반복해서 수행하면서 모델을 구성하는 최적의 파라미터를 찾음\n",
        "- 지정한 epochs 수만큼 **학습**과정과 **평가**과정을 반복하면서, 모델의 성능(loss, accuracy 등)을 체크함\n",
        "\n",
        "```\n",
        "epochs = 10\n",
        "for i in range(epochs) :\n",
        "    print(f\"Epoch {i+1} \\n------------------------\")\n",
        "    train_loop(train_dataloader, model, loss, optimizer)\n",
        "    test_loop(test_dataloader, model, loss)\n",
        "print(\"Done!\")\n",
        "```"
      ],
      "metadata": {
        "id": "kFQKm4PesSfa"
      }
    }
  ]
}