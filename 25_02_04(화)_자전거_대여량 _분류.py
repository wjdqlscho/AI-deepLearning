# -*- coding: utf-8 -*-
"""25. 02. 04(화)-자전거 대여량

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZWRSqBzt7FAZJoqhplCk4oi7encB1Yc
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('exercise2.csv')
df.head()

df.shape

df.describe()

sns.histplot(x = df['count'])

nrows, ncols = 2, 5
fig, axs = plt.subplots(nrows=nrows, ncols=ncols)
fig.set_size_inches(20,8)

for i in range(nrows):
    for j in range(ncols):
        attr = i * ncols + j + 1
        sns.histplot(x = df.columns[attr], data = df, ax=axs[i][j])

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_log_error

Y = df['count']
X = df.drop(['datetime', 'count'], axis=1,inplace=False) #독립변수로 설정

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42) # 테스트 사이즈를 30퍼센트, 트레인 셋으로 70퍼센트

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_test)

mean_squared_log_error(y_test, y_pred)

coef = pd.Series(lr_model.coef_, index=X.columns)
coef_sort = coef.sort_values(ascending=False)
sns.barplot(x=coef_sort.values, y=coef_sort.index) # 코어 이피시언트 = weight(가중치)가 가장 큰 것이 어떤 컬럼이었는지 확인을 위한
# 예측하는 데에 가장 중요한 역할을 한 것이 레지스터드와 캐주얼이었다는 것을 알 수 있음

df['datetime'] = df['datetime'].astype('datetime64')
df['year'] = df['datetime'].dt.year
df['month'] = df['datetime'].dt.month
df['day'] = df['datetime'].dt.day
df['hour'] = df['datetime'].dt.hour

df.drop(['datetime','casual','registered'], axis=1,inplace=True)

y_log = np.log1p(Y)

nrows, ncols = 1, 2
fig, axs = plt.subplots(nrows=nrows, ncols=ncols)
fig.set_size_inches(10, 4)
sns.histplot(Y, ax = axs[0])
sns.histplot(y_log, ax = axs[1])

df = pd.get_dummies(df, columns=['year', 'month', 'day', 'hour', 'holiday',
                                              'workingday','season','weather'])

df.columns

from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

lr_model = LinearRegression()
rf_model = RandomForestRegressor(random_state = 42)
xgb_model = XGBRegressor(random_state = 42)
lgbm_model = LGBMRegressor(random_state = 42)

model_list = [lr_model, rf_model, xgb_model, lgbm_model]
for model in model_list:
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    m = model.__class__.__name__
    score = mean_squared_log_error(np.expm1(y_test), np.expm1(y_pred))
    print('{0} msle: {1:.3f}'.format(m, score))
